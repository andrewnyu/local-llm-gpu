import os
import torch
from fastapi import FastAPI
from transformers import AutoModelForCausalLM, AutoTokenizer
from dotenv import load_dotenv

# âœ… Load Hugging Face token
load_dotenv()
HUGGINGFACE_TOKEN = os.getenv("HUGGINGFACE_TOKEN")

app = FastAPI()

# âœ… Choose the model (use a lightweight one for testing)
MODEL_NAME = "meta-llama/Llama-3.2-1B"  # âš¡ MUCH faster than LLaMA
#MODEL_NAME = "meta-llama/Llama-3.2-3B"  # Model to be used for production

# âœ… Auto-detect the best device
if torch.cuda.is_available():
    device = "cuda"  # âœ… Use GPU if available
    torch_dtype = torch.float16
else:
    device = "cpu"  # âœ… Use CPU if nothing else is available
    torch_dtype = torch.float32

print(f"ğŸ”¥ Using device: {device}")

# âœ… Load Model with Debugging
try:
    print(f"ğŸ” Loading tokenizer for {MODEL_NAME}...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HUGGINGFACE_TOKEN)
    print(f"âœ… Tokenizer loaded!")

    print(f"ğŸ” Loading model {MODEL_NAME} onto {device}...")
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch_dtype,
        device_map={"": device},
        token=HUGGINGFACE_TOKEN
    )
    print(f"âœ… Model loaded successfully!")
except Exception as e:
    print(f"âŒ Model failed to load: {e}")

@app.get("/query")
def query(text: str):
    """Generate text using the model."""
    print(f"ğŸ“ Received query: {text}")

    try:
        # âœ… Ensure input moves to the correct device
        inputs = tokenizer(text, return_tensors="pt").to(device)
        print(f"ğŸ”¹ Inputs prepared!")

        # âœ… Generate text with debugging
        output = model.generate(**inputs, max_new_tokens=50)
        print(f"âœ… Model inference completed!")

        # âœ… Decode response
        response = tokenizer.decode(output[0], skip_special_tokens=True)
        print(f"ğŸ“ Response: {response}")

        return {"response": response}

    except Exception as e:
        print(f"âŒ ERROR during inference: {e}")
        return {"error": str(e)}
